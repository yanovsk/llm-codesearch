{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pinecone\n",
    "import os\n",
    "pinecone.init(api_key=\"\", environment=\"\")\n",
    "index = pinecone.Index(index_name='')\n",
    "os.environ['OPENAI_API_KEY'] = \"\"\n",
    "openai.api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygments import lex\n",
    "from pygments.lexers import GoLexer\n",
    "\n",
    "def get_functions_and_imports(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    lexer = GoLexer()\n",
    "    tokens = list(lex(content, lexer))\n",
    "\n",
    "    functions = [value for ttype, value in tokens if \"Name.Function\" in str(ttype)]\n",
    "    imports = [value.replace('\"', '') for ttype, value in tokens if \"Literal.String\" in str(ttype) and value.startswith('\"')]\n",
    "\n",
    "    return functions, imports\n",
    "\n",
    "# Usage\n",
    "filename = \"./company.go\"\n",
    "functions, imports = get_functions_and_imports(filename)\n",
    "print(\"Functions:\", functions)\n",
    "print(\"Imports:\", imports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pre_process_query(user_query):\n",
    "    response = openai.Embedding.create(\n",
    "        input=user_query.strip(),\n",
    "        model=\"text-embedding-ada-002\",\n",
    "    )\n",
    "    description_embeddings = response[\"data\"][0][\"embedding\"]\n",
    "\n",
    "    pinecone_result = index.query(vector=description_embeddings,\n",
    "                                include_metadata=True,\n",
    "                                top_k=10,\n",
    "                                namespace=\"vicky\")\n",
    "    print(pinecone_result)\n",
    "\n",
    "\n",
    "    for match in pinecone_result.matches:\n",
    "        type = match.metadata[\"type\"]\n",
    "        summary = match.metadata[\"summary\"]\n",
    "        package = match.metadata[\"package\"]\n",
    "        if type in {\"function\", \"type\", \"struct\", \"method\", \"interface\"}:\n",
    "            filename = match.metadata[\"filename\"]\n",
    "            imports = match.metadata[\"imports\"]\n",
    "            code = match.metadata[\"code\"]\n",
    "            context += f\"{{ 'filename': '{filename}', 'type': {type}, 'package': '{package}', 'summary of the parent file': '{summary}', 'code': '{code}' }},\"\n",
    "        elif type == \"file\":\n",
    "            filename = match.metadata[\"filename\"]\n",
    "\n",
    "            context += f\"{{ 'filename': '{filename}', 'type': {type}, 'package': '{package}', 'summary of this file': '{summary}'  }},\"\n",
    "        elif type == \"package\":\n",
    "            context += f\"{{ 'type': {type}, 'package': '{package}', 'description of package': '{summary}'  }},\"\n",
    "    print(context)\n",
    "    prompt_with_context = f\"Answer this questions using the context from codebase (unless the question is a follow up on your previous response). {user_query} \\n Context: {context}\"\n",
    "\n",
    "    return prompt_with_context\n",
    "pre_process_query(\"explain the formula for calculating dilluted number of shares for individual security?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST QUERY\n",
    "\n",
    "QUERY =  \"explain the formula for calculating dilluted number of shares for individual security?\"\n",
    "\n",
    "response = openai.Embedding.create(\n",
    "input=QUERY,\n",
    "model=\"text-embedding-ada-002\",\n",
    ")\n",
    "embeddings_vector = response[\"data\"][0][\"embedding\"]\n",
    "\n",
    "pinecone_result = index.query(vector=embeddings_vector,\n",
    "                            include_metadata=True,\n",
    "                            filter ={\"isTest\": {\"$eq\": False}},\n",
    "                            top_k=10,\n",
    "                            namespace='vicky')\n",
    "\n",
    "                            \n",
    "print(pinecone_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "import time\n",
    "from json import JSONDecodeError\n",
    "import openai\n",
    "import pinecone\n",
    "import json\n",
    "import os\n",
    "pinecone.init(api_key=\"\", environment=\"\")\n",
    "index = pinecone.Index(index_name='')\n",
    "\n",
    "def get_tokens(string):\n",
    "  encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "  num_tokens = len(encoding.encode(string))\n",
    "  return num_tokens\n",
    "\n",
    "\n",
    "ALL_PACKAGES = ['numbers', 'cals']\n",
    "\n",
    "preprocess_functions = [{\n",
    "  \"name\": \"search_code_in_vector_db\",\n",
    "  \"description\": \"\"\"This function searches vector db for code snippets that can be used to answer user's query. It's arguments are\n",
    "   'isTest' - which must is True, if the query mentions test files, and False otherwise. \n",
    "   'package_names' is optional parameter consisting of the packages that are mentioned in user's query. If no packages are mentioned in the query it should be null.\n",
    "   'cleaned_query' Is a parameter that is used to create vector embeddings. It should be a modified original query, cleaned from all words that are not relevant to the query.\n",
    "    E.g. If original query is 'explain the formula for calculating fully diluted (as its written in the code)', then cleaned query should be 'formula for calculating fully diluted' \"\"\",\n",
    "  \"parameters\": {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "      \"isTest\": {\n",
    "        \"type\": \"boolean\",\n",
    "        \"description\": \"This argument must be set to True, if in providing answer to user's query requires searching in test files, and False otherwise.\"\n",
    "      },\n",
    "      \"package_names\": {\n",
    "        \"type\": \"array\",\n",
    "        \"description\": \"This parameters is the array of the packages that are mentioned in user's query. It should only be filled out if packages are specifically mentioned in user's query, otherwise it should be left null\",\n",
    "        'items': {\n",
    "          'type': 'string',\n",
    "          \"description\": \"This represents individual package name that is mentioned in user's query. If no packages are mentioned in the query it should be null.\", \n",
    "          \"enum\": ALL_PACKAGES\n",
    "        }\n",
    "      },\n",
    "      \"cleaned_query\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"\"\"This is the query that has been cleaned of all the packages and other unnecessary words. \n",
    "        Since this query will be embedded, it needs to be cleaned of all the unnecessary words and have maximum meaning as embedding vector .\"\"\"\n",
    "    },\n",
    "  },\n",
    "  'required': ['isTest', 'cleaned_query'],\n",
    "  }\n",
    "}]\n",
    "\n",
    "preprocess_system_prompt = \"\"\"You are a code search query processor. You will take queries from users that are interacting with a code search bot and process them to call\n",
    "    a search_code_in_vector_db function that will call vector db to find code snippets that can be used to answer user's query.\"\"\"\n",
    "\n",
    "def create_chat_completion(user_input):\n",
    "    return openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": preprocess_system_prompt\n",
    "        }, {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_input.strip(),\n",
    "        }],\n",
    "        functions=preprocess_functions,\n",
    "        function_call={\"name\": \"search_code_in_vector_db\"},\n",
    "    )\n",
    "internal_import_descriptions = {\n",
    "    \"usecases\": \"Business and Workflow logic\",\n",
    "    \"domain\": \"Simple marshalling/unmarshalling with the database + some helper methods and deprecated calculations\",\n",
    "    \"caplogic\": \"Graphql layer on top of usecases\",\n",
    "    \"api\": \"The HTTP layer of the API, contains rproxies, middlewares etc, routing etc.\",\n",
    "    \"hasura\": \"Deprecated package that houses clients and query/mutations.\",\n",
    "    \"postgres\": \"Package that contains our postgres client and some queries/helpers.\",\n",
    "    \"authx\": \"Package that contains authorization logic\",\n",
    "    \"qmail\": \"Queueing mail system package, this is our entrypoint to sendgrid\",\n",
    "    \"excel\": \"For rendering data into excel\",\n",
    "    \"numbers\": \"For creating mini-exports with correct numerical values\",\n",
    "    \"numbers/calcs\": \"For calculating every number in the system in one place\",\n",
    "    \"e2e\": \"End-to-end test suite, for tests that exercise the stack at the top level\",\n",
    "}\n",
    "\n",
    "def process_interal_packages(internal_imports):\n",
    "    internal_packages = []\n",
    "    for internal_import in internal_imports:\n",
    "        if internal_import in internal_import_descriptions:\n",
    "            internal_packages.append(f\"{internal_import} ({internal_import_descriptions[internal_import]})\")\n",
    "        else:\n",
    "            internal_packages.append(internal_import)\n",
    "    return internal_packages\n",
    "\n",
    "def process_matches(matches):\n",
    "    context = \"\"\n",
    "    json_output = [] \n",
    "\n",
    "    for match in matches:\n",
    "        match_data = {} \n",
    "        type = match.metadata[\"type\"]\n",
    "        summary = match.metadata[\"summary\"]\n",
    "        package = match.metadata[\"package\"]\n",
    "\n",
    "        if type in {\"function\", \"type\", \"struct\", \"method\", \"interface\"}:\n",
    "            filename = match.metadata[\"filename\"]\n",
    "            code = match.metadata[\"code\"]\n",
    "            internal_imports = process_interal_packages(match.metadata[\"internal_imports\"])\n",
    "            external_imports = match.metadata[\"external_imports\"]\n",
    "\n",
    "            context += f\"{{ 'type': {type}, 'code': '{code}', 'interal imports': {internal_imports}, 'external imports':  {external_imports} , 'package': '{package}', 'origin file': {filename}, 'origin file description': '{summary}' }},\"\n",
    "      \n",
    "            match_data = {\n",
    "            'type': type,\n",
    "             'code': code, \n",
    "             'interal import pkgs': internal_imports,  \n",
    "            'external import pkgs': external_imports,  \n",
    "             'package': package, \n",
    "             'origin file': filename, \n",
    "             'origin file description': summary}\n",
    "\n",
    "        elif type == \"file\":\n",
    "            filename = match.metadata[\"filename\"]\n",
    "            internal_imports = process_interal_packages(match.metadata[\"internal_imports\"])\n",
    "            external_imports = match.metadata[\"external_imports\"]\n",
    "            internal_imports = process_interal_packages(match.metadata[\"internal_imports\"])\n",
    "            function_names =  match.metadata[\"function_names\"]\n",
    "\n",
    "            context += f\"{{ 'filename': '{filename}', 'type': {type}, 'package': '{package}',  'interal imports': {internal_imports}, 'external imports':  {external_imports} , 'functions in file': {function_names}, 'file description': '{summary}'  }},\"\n",
    "\n",
    "            # Storing data for JSON file\n",
    "            match_data = {\n",
    "                'type': type,\n",
    "                'filename': filename,\n",
    "                'package': package,\n",
    "                'internal imports': internal_imports,\n",
    "                'external imports': external_imports,\n",
    "                'functions in file': function_names,\n",
    "                'file description': summary\n",
    "            }\n",
    "\n",
    "        elif type == \"package\":\n",
    "            context += f\"{{ 'type': {type}, 'package': '{package}', 'description of package': '{summary}'  }},\"\n",
    "            \n",
    "            match_data = {\n",
    "                'type': type,\n",
    "                'package': package,\n",
    "                'package description': summary\n",
    "            }\n",
    "        \n",
    "        json_output.append(match_data)  # appending individual match data to the list\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists('context_data.json'):\n",
    "        with open('context_data.json', 'r') as json_file:\n",
    "            existing_data = json.load(json_file)\n",
    "            json_output.extend(existing_data)\n",
    "\n",
    "    # Writing the JSON output to a file\n",
    "    with open('context_data.json', 'w') as json_file:\n",
    "        json.dump(json_output, json_file, indent=4)\n",
    "\n",
    "    return context\n",
    "\n",
    "\n",
    "def index_query(vector, top_k, filter, namespace):\n",
    "    query = index.query(vector=vector,\n",
    "                       include_metadata=True,\n",
    "                       top_k=top_k,\n",
    "                       filter=filter,\n",
    "                       namespace=namespace)\n",
    "    return query\n",
    "\n",
    "\n",
    "\n",
    "def query_to_func_args(user_input):\n",
    "    isTest, cleaned_query, packages = False, user_input, ALL_PACKAGES\n",
    "\n",
    "    for i in range(6):\n",
    "        try:\n",
    "            first_response = create_chat_completion(user_input)\n",
    "            response_message = first_response[\"choices\"][0][\"message\"]\n",
    "\n",
    "            if response_message.get(\"function_call\"):\n",
    "                arguments = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
    "\n",
    "                if arguments.get(\"isTest\"):\n",
    "                    isTest = arguments[\"isTest\"]\n",
    "\n",
    "                if arguments.get(\"package_names\"):\n",
    "                    packages = arguments[\"package_names\"]\n",
    "\n",
    "                if arguments.get(\"cleaned_query\"):\n",
    "                    cleaned_query = arguments[\"cleaned_query\"]\n",
    "\n",
    "                return isTest, cleaned_query, packages\n",
    "\n",
    "        except JSONDecodeError as e:\n",
    "            print(f'JSONDecodeError occurred: {e}')\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "\n",
    "    return first_response\n",
    "\n",
    "def get_context_from_vectordb(embeddings_vector, filter):\n",
    "  filter.update({\"scope\": {\"$in\": [\"low\"]}})\n",
    "\n",
    "  with open(\"functions.json\", \"r\") as f:\n",
    "    functions_list = json.load(f)\n",
    "                    \n",
    "  pinecone_res = index.query(vector=embeddings_vector,\n",
    "                      include_metadata=True,\n",
    "                      top_k=8,\n",
    "                      filter=filter,\n",
    "                      namespace=\"vicky\")\n",
    "\n",
    "  context = \"\"\n",
    "  for i, match in enumerate(pinecone_res.matches):\n",
    "    match_data = {} \n",
    "    type = match.metadata[\"type\"]\n",
    "    package = match.metadata[\"package\"]\n",
    "    filename = match.metadata[\"filename\"]\n",
    "    code = match.metadata[\"code\"]\n",
    "    internal_imports = process_interal_packages(match.metadata[\"internal_imports\"])\n",
    "    external_imports = match.metadata[\"external_imports\"]\n",
    "\n",
    "    context += f\"## {i+1}) Relevant {type} ##: \"\n",
    "    context += f\"{{'code': '{code}', 'interal file imports': {internal_imports}, 'external file imports':  {external_imports} , 'package': '{package}', 'origin file': {filename}}},\\n\"\n",
    "\n",
    "    functions_called = match.metadata[\"functions_called\"]\n",
    "\n",
    "    if functions_called:\n",
    "      context += f\"#Code of functions called by #: \"\n",
    "      for function_call in functions_called:\n",
    "        try:\n",
    "          if functions_list[function_call]:\n",
    "            context += f\"{{'code': '{functions_list[function_call]}'}}, \\n\"\n",
    "        except:\n",
    "          print('no func found')\n",
    "    print(context)\n",
    "  return context\n",
    "\n",
    "\n",
    "\n",
    "def gpt_pre_process_query(user_query):\n",
    "    isTest, cleaned_query, packages = query_to_func_args(user_query)\n",
    "\n",
    "    embeddings_vector = openai.Embedding.create(\n",
    "        input=cleaned_query,\n",
    "        model=\"text-embedding-ada-002\",\n",
    "    )[\"data\"][0][\"embedding\"]\n",
    "\n",
    "    print(packages)\n",
    "\n",
    "    if not isTest: \n",
    "      filter = {\"isTest\": {\"$eq\": False}}\n",
    "    else:  \n",
    "      filter = {}\n",
    "\n",
    "    context = get_context_from_vectordb(embeddings_vector, filter)\n",
    "\n",
    "    prompt_with_context = f\"Answer this questions using the context from codebase (unless the question is a follow up on your previous response). {user_query} \\n Context: {context}\"\n",
    "    return prompt_with_context\n",
    "\n",
    "\n",
    "system_prompt = \"You are helpful codebot that gets user query and a context from a golang BE codebase that must be used to answer user's query. You must analyze the code and provide a clear answer to the query.\"\n",
    "\n",
    "def handle_user_query(user_input, messages_history):\n",
    "\n",
    "  messages = [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": system_prompt\n",
    "    },\n",
    "  ]\n",
    "\n",
    "  for msg in messages_history:\n",
    "    role, content = msg\n",
    "    if get_tokens(content) > 5000:\n",
    "      content = content[:10000]\n",
    "\n",
    "    messages.append({\n",
    "      \"role\": role,\n",
    "      \"content\": content,\n",
    "    })\n",
    "\n",
    "  messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": user_input,\n",
    "  })\n",
    "\n",
    "  for i in range(6):\n",
    "    try:\n",
    "      response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4-32k\",\n",
    "        messages=messages,\n",
    "      )\n",
    "      return response[\"choices\"][0][\"message\"]['content']\n",
    "\n",
    "    except JSONDecodeError as e:\n",
    "      print(f'JSONDecodeError occurred: {e}')\n",
    "      time.sleep(1)\n",
    "      continue\n",
    "\n",
    "\n",
    "QUERY = \"explain the formula for calculating fully diluted (as it's written in the code)\"\n",
    "query_with_context = gpt_pre_process_query(QUERY)\n",
    "\n",
    "response = handle_user_query(query_with_context, [])\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"functions.json\", \"r\") as f:\n",
    "    functions_list = json.load(f)   \n",
    "\n",
    "functions_list['isZero']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"functions.json\", 'r' ) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "lowercased_data = {k.lower(): v for k, v in data.items()}\n",
    "\n",
    "with open(\"functions.json\", 'w' ) as f:\n",
    "    f.write(json.dumps(lowercased_data, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
